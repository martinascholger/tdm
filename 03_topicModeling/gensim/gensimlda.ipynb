{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7dc221",
   "metadata": {},
   "source": [
    "# Topic Modeling mit Gensim\n",
    "\n",
    "Dieses Skript erstellt ein Topic Model mit Gensim. \n",
    "\n",
    "* **gensim** ist eine Bibliothek, die das Topic Model berechnet\n",
    "* **nltk** oder **spacy** um Text vorzuverarbeiten\n",
    "* **os** ist eine Bibliothek, um das Korpus zu laden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9aa1d",
   "metadata": {},
   "source": [
    "## Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import von Bibliotheken, die wir verwenden\n",
    "\n",
    "import gensim, nltk, os, spacy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Files ausgeben, die analysiert werden\n",
    "\n",
    "files = sorted(os.listdir('fedpapers'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung von Variablen, die unser Input-Korpus enthalten\n",
    "texts = [] # Liste der Texte\n",
    "labels = [] # Labels mit den Dateititeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fa501",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Spacy Modell muss installiert sein, z.B. en_core_web_md, siehe spacy-Modelle für Englisch: https://spacy.io/models/en\n",
    "# Installation über die Powershell \"python -m spacy download en_core_web_md\"\n",
    "\n",
    "# Spacy-Modell und Stopwörter laden\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n",
    "\n",
    "#https://machinelearningknowledge.ai/tutorial-for-stopwords-in-spacy/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fe482",
   "metadata": {},
   "source": [
    "## Vorverarbeitung der Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03726c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verwendung der os-Bibliothek, um die Files zu laden. \n",
    "# Iteriert über alle Dokumente im Korpus-Folder.\n",
    "for root, dirs, files in os.walk('fedpapers'): \n",
    "    for file_name in files: # iteriert über jede Datei, lädt und öffnet sie in runtime \n",
    "        with open(os.path.join(root, file_name), encoding='utf-8') as rf: #plattformunabhängiger Weg, um die Files zu öffnen\n",
    "            text = nlp(rf.read()) #String aus der Datei bekommen, weil wir den Text in individuelle Tokens teilen wollen.\n",
    "            cleaned = [token.text for token in text if not token.is_punct ] #Textbereinigung mit spacy\n",
    "            texts.append(cleaned)\n",
    "            labels.append(file_name[:-4]) #speichert die Filenamen (ohne Dateinamen .txt)\n",
    "\n",
    "# Ersten Text ausgeben    \n",
    "print(texts[:1])\n",
    "\n",
    "# Alternativ dazu kann zur Textbereinigung auch NLTK verwendet werden\n",
    "#            tokens = nltk.word_tokenize(text)\n",
    "#            cleaned = [word for word in tokens if word.isalnum()] \n",
    "#            texts.append(cleaned)\n",
    "#            labels.append(file_name[:-4]) #speichert Information die es erlaubt zu labeln; letzten vier Zeichen werden weggeschnitten.\n",
    "#print(texts[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff1fa8",
   "metadata": {},
   "source": [
    "## Topic Modeling \n",
    "\n",
    "Verwendung von gensim, einer Python Bibliothek für Topic Modeling, Dokumentenindizierung, etc., um das Topic Modeling durchzuführen. Dabei wird das Korpus in ein gensim.corpora.Dictionary transformiert, um es anschließend mit Gensim verarbeiten zu können. \n",
    "\n",
    "Zunächst muss gensim über die Powershell installiert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48455b66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac567cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Erstellt ein Gensim dictionary aus den Texten\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Konvertiert dictionary in ein bag-of-words Korpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Erstellt LDA model unter Verwendung von corpus und dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None \n",
    "\n",
    "# Index zum Wort dictionary.\n",
    "temp = dictionary[0]  # Laden des dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus, # das Korpus\n",
    "    id2word=id2word,# mapping der id-Zahlen zu den Wörtern\n",
    "    chunksize=chunksize, #Chunkgröße\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,#Anzahl der Iterationen\n",
    "    num_topics=num_topics,Anzahl der Topics\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"topicmodel-lda.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057370d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bei der Erstellung des Topic Models wurden im Folder Dateien mit dem Präfix fed_ gespeichert, z.B. das Korpus, die Topicverteilung in den Dokumenten, die Top-Keywords in den Topics etc. Diese Dateien können zur Weiterverarbeitung und Visualisierung verwendet werden.\n",
    "# Die Informationen können aber auch direkt hier angezeigt werden, z.B. die Topics mit den Top 20 Wörtern\n",
    "topics = model.show_topics(num_topics=num_topics, num_words=20)\n",
    "print (topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ee55b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hier etwas übersichtlicher\n",
    "for topic in topics:\n",
    "    print(f\"Topic #{topic[0]}: {topic[1]}...\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1f99b",
   "metadata": {},
   "source": [
    "### Visualisierungsmöglichkeiten\n",
    "Der letzte Teil des Topic Modeling Workflows besteht aus der Visualisierung der Ergebnisse. Es gibt unzählige Visulisierungen, die wiederverwendet und angepasst werden können, z.B. hier: https://www.youtube.com/watch?v=GLfOANUqulA&t=19s und https://www.youtube.com/watch?v=GLfOANUqulA&t=19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee4293e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Wordcloud erstellen\n",
    "\n",
    "# Dazu muss zunächst über die Powershell wordcloud installiert werden mit pip install wordcloud\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10,10), sharex=True, sharey=True) # Anzahl der Plots kann hier erhöht werden. \n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('wordcloud.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ac452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
