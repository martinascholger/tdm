{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae5a7ee",
   "metadata": {},
   "source": [
    "### Inter-rater agreement kalkulieren\n",
    "\n",
    "Dieses Notebook kalkuliert das inter-rater agreement aus einer Annotationsstudie, die von den Teilnehmer:innen der Lehrveranstaltung im WS 2023 durchgeführt wurde. Die Excel-Datei \"newspaper_sentiment_migration.csv\" enthält 24 Sätze aus den Zeitungen \"Das Vaterland\" (Zeitung für die österreichische Monarchie, 1860-1911) und \"Neue Freie Presse\" (1864-1939). Es wurde jeweils der Satz aus der Spalte \"To Annotate\" mit einem Polaritätswert (pos, neu oder neg) annotiert. Um den Satz im Kontext lesen zu können, wird auch der vorherige bzw. nachfolgende Satz zur Verfügung gestellt, sofern dieser existiert. \n",
    "\n",
    "Projektkontext: Lucija Krusic untersucht in ihrer Dissertation den Sentiment gegenüber MigrantInnen in österreichischen Zeitungen. \n",
    "\n",
    "- **Inter-rater agreement**: Berechnet die Übereinstimmung der Annotationen zwischen verschiedenen Annotator:innen. \n",
    "- **Intra-rater agreement**: Bezieht sich auf die Konsistenz mehrerer Annotationsdurchläufe einer einzigen Person über die Zeit.\n",
    "\n",
    "Metriken zur Kalkulation von Übereinstimmungen: \n",
    "- **Cohen's kappa**: Cohens Kappa ist eine statistische Maßzahl, die verwendet wird, um das Ausmaß der Übereinstimmung (Agreement) zwischen zwei Bewertungen zu messen. Cohens Kappa wird häufig bei kategorialen Daten verwendet. \n",
    "\n",
    "Weitere Metriken sind: **Fleiss' kappa** oder **Krippendorf's Alpha**\n",
    "\n",
    "Cohen's kappa agreements:\n",
    " - < 0.00 = Poor\n",
    " - 0.00-0.20 = Slight\n",
    " - 0.20-0.40 = Fair\n",
    " - 0.41-0.60 = Moderate\n",
    " - 0.61-0.80 = Substantial\n",
    " - 0.81-1.00 = Almost perfect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d659940",
   "metadata": {},
   "source": [
    "## Dateien importieren\n",
    "\n",
    "1. Import des glob-Moduls für die Suche nach Dateien mit einem bestimmten Namensmuster innerhalb eines Verzeichnisses.\n",
    "2. Pfad zum Folder definieren in der Variable `folder_path`\n",
    "3. Die Funktion `glob.glob()`liefert eine Liste von Dateipfaden zurück, die einem bestimmten Muster entsprechen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9923d3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"annotationen/\"\n",
    "\n",
    "file_paths = glob.glob(folder_path + \"*.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb7229",
   "metadata": {},
   "source": [
    "Im nächsten Schritt werden \n",
    "- Dateipfade identifiziert um zu überprüfen, ob die Daten überhaupt erreichbar sind\n",
    "- Ein leeres DataFrame erstellt um Daten aus allen Dateien kombinieren zu können\n",
    "- Jedes Excelfile eingelesen und der inhalt dem `combined_data` Dateframe hinzugefügt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef4a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Identified file paths:\", file_paths)\n",
    "\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "for file in file_paths:\n",
    "    df = pd.read_excel(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729886dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a3c57",
   "metadata": {},
   "source": [
    "Exkurs: Lädt die Daten und überprüft, ob der Wert in der Spalte Polarität valide ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a89dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "invalid_values = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_excel(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "    invalid_entries = df[~df[\"Polarität (pos/neu/neg)\"].isin([\"neg\", \"pos\", \"neu\"])]\n",
    "    if not invalid_entries.empty:\n",
    "        invalid_values[os.path.basename(file_path)] = invalid_entries\n",
    "\n",
    "invalid_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde31f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552e59f",
   "metadata": {},
   "source": [
    "Extrahiert die Sätze und die korrespondierenden Annotationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6049a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_annotations = {}\n",
    "for df in data_frames:\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row[\"To annotate\"]\n",
    "        annotation = row[\"Polarität (pos/neu/neg)\"]\n",
    "        if sentence not in sentences_annotations:\n",
    "            sentences_annotations[sentence] = []\n",
    "        sentences_annotations[sentence].append(annotation)\n",
    "\n",
    "# Filtert Sätze, die nicht von allen annotiert wurden. \n",
    "fully_annotated_sentences = {sentence: annotations for sentence, annotations in sentences_annotations.items() if len(annotations) == len(data_frames)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kalkuliert Cohen's kappa für jedes Paar an Annotator:innen\n",
    "annotators = range(len(data_frames))\n",
    "kappa_scores = np.zeros((len(data_frames), len(data_frames)))\n",
    "\n",
    "for i, j in itertools.combinations(annotators, 2):\n",
    "    annotations_i = [fully_annotated_sentences[sentence][i] for sentence in fully_annotated_sentences]\n",
    "    annotations_j = [fully_annotated_sentences[sentence][j] for sentence in fully_annotated_sentences]\n",
    "    kappa = cohen_kappa_score(annotations_i, annotations_j)\n",
    "    kappa_scores[i][j] = kappa\n",
    "    kappa_scores[j][i] = kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiert die Resultate\n",
    "annotator_labels = [f'Annotator {i+1}' for i in annotators]\n",
    "sns.heatmap(kappa_scores, annot=True, fmt=\".2f\", xticklabels=annotator_labels, yticklabels=annotator_labels)\n",
    "plt.title(\"Cohen's Kappa Scores Between Annotators\")\n",
    "plt.show()\n",
    "\n",
    "# Berechnet durchschnittlichen Cohen's Kappa-Wert\n",
    "average_kappa = np.sum(kappa_scores) / (len(annotators) * (len(annotators) - 1))\n",
    "average_kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ad30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
