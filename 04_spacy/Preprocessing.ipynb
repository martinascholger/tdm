{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorverarbeitung von Texten mit spaCy\n",
    "\n",
    "Textdateien müssen für die Weiterverarbeitung häufig vorab bereinigt bzw. entsprechend vorbereitet werden, um sie für unterschiedliche Anwendungsfälle nutzbar zu machen. \n",
    "\n",
    "Für Python existiert eine große Auswahl an Bibliotheken für Natural Language Processing (algorithmische Verarbeitung von Text- und Sprachdaten), die sich bezüglich ihres Funktionsumfanges und ihrer Performance teilweise deutlich unterscheiden. Die gängisten Bibliotheken sind: \n",
    "\n",
    "* [NLTK](http://www.nltk.org/)\n",
    "* [spaCy](https://spacy.io/)\n",
    "* [openNLP](https://opennlp.apache.org/)\n",
    "* [CoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n",
    "\n",
    "Hier ein kurzer Einblick in spaCy, eine Bibliothek zur Tokenisierung, Named Entity Recognition, Deep Learning Integration, Unterstützung von 61 Sprachen etc.  \n",
    "* Einfacher Einstieg in spaCy [spacy 101](https://spacy.io/usage/spacy-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# zunächst über powershell Sprachpaket installieren:\n",
    "# python -m spacy download en_core_web_md\n",
    "# python -m spacy download de_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_md\")\n",
    "#doc = nlp(\"‘New dawn in America’: World leaders welcome U.S. President Joe Biden and Vice President Kamala Harris.\")\n",
    "#for token in doc:\n",
    "#    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir waren z.B. früher auf'm Fahrrad unterwegs in Graz (immer nach 11 Uhr).\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "with open('fahrrad.txt', encoding='utf-8') as f:\n",
    "        doc = nlp(f.read())\n",
    "        print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir\n",
      "waren\n",
      "z.B.\n",
      "früher\n",
      "auf\n",
      "'m\n",
      "Fahrrad\n",
      "unterwegs\n",
      "in\n",
      "Graz\n",
      "(\n",
      "immer\n",
      "nach\n",
      "11\n",
      "Uhr\n",
      ")\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisieren: https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir\n",
      "waren\n",
      "z.B.\n",
      "früher\n",
      "auf\n",
      "'m\n",
      "Fahrrad\n",
      "unterwegs\n",
      "in\n",
      "Graz\n",
      "immer\n",
      "nach\n",
      "11\n",
      "Uhr\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.is_punct == False:\n",
    "        # Zahlen entfernen\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text in Kleinbuchstaben ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stoppwörter entfernen und Text normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.B.  :  z.B.\n",
      "Fahrrad  :  Fahrrad\n",
      "Uhr  :  Uhr\n"
     ]
    }
   ],
   "source": [
    "#fileNoun = open(\"fahrrad-noun.txt\", \"w\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        print(token, ' : ', token.lemma_) #file=fileNoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graz LOC\n"
     ]
    }
   ],
   "source": [
    "# Entitäten erkennen\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
